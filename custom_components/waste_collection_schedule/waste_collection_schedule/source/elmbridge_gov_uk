from email import header
import logging
import requests

from bs4 import BeautifulSoup
from datetime import datetime, timedelta
# from time import sleep
from waste_collection_schedule import Collection

TITLE = 'elmbridge.gov.uk'
DESCRIPTION = (
    'Source for waste collection services for elmbridge Borough Council'
)
URL = 'https://www.elmbridge.gov.uk/waste-and-recycling/'


HEADERS = {
    "user-agent": "Mozilla/5.0",
}

TEST_CASES = {
    "Test_001" : {"uprn": "100030491624"},
    "Test_002": {"uprn": "100030491614"},
    "Test_003": {"uprn": "100030493289"},
    "Test_004": {"uprn": "200001136341"}
}

API_URLS = {
    'session': 'https://emaps.elmbridge.gov.uk/myElmbridge.aspx',
    'search': 'https://emaps.elmbridge.gov.uk/myElmbridge.aspx?action=SetAddress&UniqueId={}',
    'schedule': 'https://emaps.elmbridge.gov.uk/myElmbridge.aspx?tab=0#Refuse_&_Recycling',
}

ICONS = {
    "REFUSE": "mdi:trash-can",
    "RECYCLING": "mdi:recycle",
    "FOOD BINS": "mdi:leaf",
    "GARDEN": "mdi:leaf",
}


_LOGGER = logging.getLogger(__name__)

class Source:
    def __init__(self, uprn: str):
        self._uprn = str(uprn)

    def fetch(self):
        # Collection dates returned do not contain a year, so assume they are for the current year.
        # This'll cause problems in December as upcoming January collections will have been assigned dates in the past.
        # Some clunky logic can deal with this:
        #   If a date in less than 1 month in the past, it doesn't matter as the collection will have recently occured.
        #   If a date is more than 1 month in the past, assume it's an incorrectly assigned date and increment the year by 1.
        # Better ideas welcome!

        today = datetime.now()
        today = today.replace(hour = 0, minute = 0, second = 0, microsecond = 0)
        year = today.year

        s = requests.Session(header = HEADERS)

        r0 = s.get(API_URLS['session'])
        r0.raise_for_status()
        r1 = s.get(API_URLS['search)'].format(self._uprn))
        r1.raise_for_status()
        r2 = s.get(API_URLS['schedule'])
        r2.raise_for_status()
        responseContent = r2.text

        soup = BeautifulSoup(responseContent, 'html.parser')

        entries = []
        for tr in soup.findAll('tr'):
            row = []
            for td in tr.findAll('td'):
                row.append(td.text.strip())
            row.pop(1)  # removed superflous element
            dt = row[0] + ' ' + str(year)
            dt = datetime.strptime(dt, '%d %b %Y')
            if (dt - today) < timedelta(month = -1):
                dt = dt.replace(year = dt.year + 1)
            row[0] = dt

            wastetypes = row[1].split(' + ')
            for waste in wastetypes:
                entries.append(
                    Collection(
                        date = row[0],
                        t = waste + ' bin',
                        icon = ICONS.get(waste.upper())
                    )
                )

        return entries
